\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Results}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Fold Perfomance}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Implementation Details}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Questions}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Discuss how you obtained the optimal topology and optimal values of network parameters. Describe the performance measure you used (and explain why you preferred it over other measures) and the different topologies / parameters you experimented with}{1}}
\csname bt@set@cnt\endcsname{0}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Explain what strategy you employed to ensure good generalisation ability of the networks and overcome the problem of overfitting}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}In Part VIII you used the optimal parameters that you found in part VI to train your networks. However, there is a problem with this approach, the data you used for validation at some point will be used for testing in cross-validation. Can you explain why this a problem? Ideally how should you optimise the parameters in cross-validation?}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Is there any difference in the classification performance of the two different classification approaches. Discuss the advantages / disadvantages of using 6 single-outut NNs vs. 1 six-output NNs}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Code Flow Chart}{2}}
